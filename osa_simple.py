#!/usr/bin/env python3
"""
Simplified MemCore for testing - Direct Ollama integration
"""

import asyncio
import ollama
from typing import Dict, Any, Optional

class SimpleMemCore:
    """Simplified MemCore with direct Ollama integration."""
    
    def __init__(self, model: str = "llama3.2:3b"):
        self.model = model
        self.client = ollama.Client()
        self.context = []
        print(f"ü§ñ MemCore initialized with model: {model}")
    
    async def initialize(self):
        """Initialize MemCore systems."""
        print("üöÄ Starting MemCore systems...")
        # Check if model exists
        try:
            models = self.client.list()
            model_names = [m['name'] for m in models['models']]
            if self.model in model_names:
                print(f"‚úÖ Model {self.model} ready")
            else:
                print(f"‚ö†Ô∏è Model {self.model} not found, available: {model_names}")
        except Exception as e:
            print(f"‚ùå Error checking models: {e}")
    
    async def think(self, prompt: str) -> str:
        """Process a thought/task with the LLM."""
        try:
            print(f"üß† Thinking about: {prompt[:50]}...")
            
            # Use Ollama to generate response
            response = self.client.generate(
                model=self.model,
                prompt=prompt,
                context=self.context if self.context else None
            )
            
            # Update context for continuity
            if 'context' in response:
                self.context = response['context']
            
            return response['response']
            
        except Exception as e:
            return f"Error: {e}"
    
    async def accomplish_task(self, task: str) -> str:
        """Accomplish a given task."""
        print(f"\nüìù Task: {task}")
        
        # Add some thinking process
        thoughts = [
            f"Understanding the task: {task}",
            "Breaking it down into steps",
            "Generating solution"
        ]
        
        for thought in thoughts:
            print(f"   üí≠ {thought}")
            await asyncio.sleep(0.5)
        
        # Get actual response from LLM
        result = await self.think(task)
        
        return result
    
    async def shutdown(self):
        """Shutdown MemCore."""
        print("\nüëã MemCore shutting down...")
        self.context = []


async def main():
    """Test the simplified MemCore."""
    print("="*60)
    print("üéØ OmniMind Simple MemCore Test")
    print("="*60)
    
    # Initialize MemCore
    osa = SimpleMemCore(model="llama3.2:3b")
    await osa.initialize()
    
    # Interactive mode
    print("\nüí¨ Interactive mode (type 'exit' to quit)")
    print("-"*40)
    
    while True:
        try:
            task = input("\n> ").strip()
            
            if task.lower() in ['exit', 'quit']:
                break
            
            if not task:
                continue
            
            result = await osa.accomplish_task(task)
            print(f"\n‚úÖ Result:\n{result[:500]}...")  # Limit output length
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"‚ùå Error: {e}")
    
    await osa.shutdown()
    print("\n‚ú® Done!")


if __name__ == "__main__":
    asyncio.run(main())